# Understanding Response Rates

In this section, we'll tackle the challenge of outlining and explaining a methodology that makes the most out of survey data with low response rates without creating misleading or inaccurate results. Our discussion begins with a brief overview of the relationship between sample size and estimate accuracy, and then moves on to a discussion of the possible approaches to working with small samples and their relative merits and drawbacks.

:::{.callout-note}
## the current state

CLC's current approach to working with small samples is to discard estimates from small samples and replace them with either the last available estimate from a larger sample, or with an overall estimate for the local population. This approach is conceptually straightforward, but it has a few drawbacks: 

1. It discards information from the small sample, which is a waste of the effort that went into collecting it.
2. It assumes that the last available estimate from a larger sample is still accurate, which may not be the case if the population has changed in the intervening period.
3. Assessment and replacement are currently performed manually, which is time consuming and error prone. [^error-prone]
:::

[^error-prone]: the 'error-prone' descriptor, used here and elsewhere, should not be taken as a slight against the CLC evaluation team. Manual processes are inherently error-prone, requiring time and energy-intensive manual checks to ensure that the process was carried out correctly. 

## sampling error

To illustrate the relationship bewteen sample size and sampling error, we'll return to the simulated learners from the previous section. 
Stats courses on the subject will often begin with abstract examples, like drawing marbles from a bag, or flipping coins, but I've never found those particularly helpful, especially when we have a more concrete example available.
Below, for a range of sample sizes between 5 and 2500, we draw and summarize 200 samples of that size from the simulated learners.
For each sample of size $n$ (which we can think of as a set of $n$ survey responses), we calculate the representation of each relevant group in the sample, and compare that to the 'true' representation of that group in the population. 

```{r sampling}
#| caption: calculate the sampling error for different sample sizes
#| fig-label: sampling-error

sim_sample <- function(n, k) {
    seq_len(k) |>
        map(
            ~ learners |>
                sample_n(n, replace = FALSE) |>
                calculate_cwss() |>
                mutate(run = .x)
        ) |>
        bind_rows()
}

cwss_long <- cwss_summary |>
    tidyr::pivot_longer(
        everything(),
        names_to = "var",
        values_to = "val"
    )

cwss_validation <- cwss_long$val |>
    set_names(cwss_long$var)

samples <-
    tibble::tibble(
        size = c(5, 10, 50, 100, 500, 1000, 2500),
        samples = map(size, sim_sample, k = 200)
    ) |>
    unnest(samples)


sample_errors <- samples |>
    pivot_longer(
        c(-run, -size),
        names_to = "var",
        values_to = "estimate"
    ) |>
    mutate(
        var = forcats::fct_inorder(var),
        population_value = cwss_validation[var],
        empirical_error = estimate - population_value,
        standard_error = (1 - estimate) * estimate / sqrt(size),
    ) |>
    group_by(size) |>
    mutate(
        mean_absolute_deviation = mean(abs(empirical_error)),
        mean_se = mean(standard_error)
    ) |>
    ungroup() |>
    mutate(size = factor(size))



sample_errors |>
    filter(!var %in% c("cwss_actual", "cwss_error"), !(size == "5" & var == "physical_disability")) |>
    ggplot(aes(x = size, y = empirical_error, fill = mean_absolute_deviation)) +
    facet_grid(cols = vars(var)) +
    ggdist::stat_slab(expand = T, trim = FALSE, height = 3, normalize = "groups", orientation = "vertical") +
    bptheme::theme_blueprint() +
    bpscales::scale_fill_blueprint(
        type = "multi",
        option = "fsc",
        limits = c(0, .15),
        breaks = c(0, .05, .1, .15),
        guide = guide_colourbar(barwidth = 25, title.vjust = 1)
    ) +
    scale_y_continuous(limits = c(-1, 1), breaks = c(-1, -.5, -.25, -.1, -.05, 0, .05, .1, .25, .5, 1))
```

@sampling-error shows the distribution of errors (the difference between the sample's proportion and the population's) for each sample size, and for each population group, including the calculated `cwss_score`.
Distributions more tightly clustered around zero indicate less error, and thus more accurate estimates of representation in the population. 
The 'slabs' representing the distributions of errors are coloured to indicate the average absolute error for that sample size and population group. 
The average absolute error is the average of the absolute value of the error, and is a measure of the average distance between the sample's estimate and the population's true value.

The most prominent and obvious conclusion to draw is intuitive and unsurprising: larger samples yield more accurate estimates of population values. However, there are a few other important and interesting things to note: 

- Small samples are not entirely uninformative. Even samples of size 5, which are the smallest we consider here, provide some information about the population, which we can see by the fact that their average absolute error is less than .15, which is much smaller than the average absolute error we would expect if we guessed randomly (which would be .5).
- Compared to the individual characteristics, the `cwss_score` is much more stable across sample sizes. This is because the `cwss_score` is a composite of all the individual characteristics, and thus is less sensitive to the random variation in the sample.
- The absolute size of the sample (e.g. 5, 10, 50) is much more important to the accuracy of the estimates it produces than is the relative size of the sample (e.g. .1%, .2%, 1%). This is why classical formulations of the margin of error (which we'll get to later) depend only on the size of the sample, and not on the size of the population. 


```{r confidence-ranges}
#| fig-label: confidence-ranges
sample_errors |>
    filter(!var %in% c("cwss_actual", "cwss_error"), !(size == "5" & var == "physical_disability")) |>
    ggplot(aes(x = size, y = empirical_error)) +
    facet_grid(cols = vars(var)) +
    ggdist::stat_pointinterval() +
    bptheme::theme_blueprint() +
    scale_y_continuous(limits = c(-1, 1), breaks = c(-1, -.5, -.25, -.1, -.05, 0, .05, .1, .25, .5, 1))


```

### the margin of error

One of the most familiar situations in which we encounter sampling error is in the reporting of public opinion polls.
In this context, the margin of error is a measure of the uncertainty in the estimate of the proportion of the population that holds a particular opinion.
The margin of error is calculated using the sample size and the proportion of the sample that holds the opinion in question. 
It is calculated as the product of the standard error and a constant, which is determined by the desired level of confidence.

For a proportion (like our estimates of the representation of different groups in the population), the standard error is calculated as $\sqrt{\frac{p(1-p)}{n}}$, where $p$ is the proportion of the sample that holds the opinion in question, and $n$ is the sample size.
The constant is determined by the desired level of confidence, and is calculated as $z \times \sqrt{\frac{p(1-p)}{n}}$, where $z$ is the z-score associated with the desired level of confidence. For the standard 95% level of confidence, $z$ is 1.96. The relationship between $z$ and the desired level of confidence is always the same.

```{r margin-of-error}
#| fig-label: margin-of-error
#| fig-caption: margin of error for different sample sizes at 95% confidence and 50% representation in the sample

moe <- expand_grid(
    size = c(5, 10, 50, 100, 500, 1000, 2500),
    p = c(.1, .5)
)

position_nudgedodge <- function(nudge_x = 0, nudge_y = 0, width = 0.75) {
    ggproto(NULL, PositionNudgeDodge, nudge_x = nudge_x, nudge_y = nudge_y, width = width)
}

PositionNudgeDodge <- ggproto("PositionNudgeDodge", PositionDodge,
    nudge_x = 0, nudge_y = 0, width = .75,
    setup_params = function(self, data) {
        l <- ggproto_parent(PositionDodge, self)$setup_params(data)
        append(l, list(x = self$nudge_x, y = self$nudge_y))
    },
    compute_layer = function(self, data, params, layout) {
        d <- ggproto_parent(PositionNudge, self)$compute_layer(data, params, layout)
        d <- ggproto_parent(PositionDodge, self)$compute_layer(d, params, layout)
        d
    }
)

moe |>
    mutate(
        se = sqrt(p * (1 - p) / size),
        moe = 1.96 * se
    ) |>
    ggplot(aes(y = factor(size))) +
    ggdist::stat_halfeye(
        aes(
            xdist = distributional::dist_normal(mean = p, sd = se)
        ),
        position = position_dodge(width = .3)
    ) +
    geom_text(
        aes(x = p, group = p, label = glue::glue("{round(100 * p)}% +/- {round(moe * 100, 1)}")),
        position = position_nudgedodge(nudge_y = -.1, width = .3)
    ) +
    scale_x_continuous(breaks = c(0, .1, .5, 1)) +
    labs(x = "proportion", y = "sample size") +
    bptheme::theme_blueprint()
```

@margin-of-error shows the margin of error for different sample sizes at 95% confidence for both 10% and 50% representation in the sample.
Directionally, the margin of error agrees with the results of our survey simulations presented in @sampling-error, but the formula produces somewhat more conservative (larger) estimates of the error. 

:::{.callout-note}
## How much error is too much error? 

The answer to this question is a matter of judgement, and depends on the details of how CLC's reporting will be used, interrogated, and judged both internally by program managers, and externally by funders and other stakeholders.
Both @sampling-error and @margin-of-error demonstrate that samples of size 5 or 10 risk producing wildly incorrect (and therefore probably unacceptable) estimates, but that samples of size 100 or greater are quite likely to produce estimates within .1 (ten percentage points) of the true value.

Public opinion polls typically design their sampling and data collection strategies to produce estimates with a maximum margin of error (where the sample value is 50%) of no more than 3 or 4 percentage points, which requires a sample size of roughly 1000. CLC's need for accuracy is probably not as great as that of a public opinion poll, but it may still be useful to set a benchmark in terms of the maximum acceptable margin of error. 
:::



## borrowing information from previous months

CLC's evaluation team has discussed their struggles with low response rates in the context of individual surveys administered within a given reporting period. However, CLC has the advantage of being able to combine results from multiple reporting periods to increase the sample size available for analysis. The current practice of discarding estimates from small samples and replacing them with the last available estimate from a larger sample prefigures this approach, but we can do better by using all the available data to produce a more accurate and current estimate. 

### a simple moving average

The simplest way to combine estimates from multiple reporting periods is to pool responses from some number of previous reporting periods, and then calculate the proportion of each group in the pooled sample. 
To illustrate the benefits of this approach, we'll arbitrarily divide our simulated population of learners into 12 reporting periods, draw 'batches' of small samples from each, calculate moving averages with different sizes of window, and then observe the error. 
For the sake of simplicity, we'll focus on estimating the representation of `women_and_beyond` learners in the population, but the approach and the results are applicable to any other group. 

To begin, I'll divide our simulated learners into 12 'monthly' reporting periods by adding a column to the data frame that indicates which reporting period each learner belongs to. 
The term period refers to some unit of time, which could be a month, a quarter, or a year, depending on the reporting cycle. 
I'll keep the numeric period value, but label each period with a month name for the sake of concreteness and clarity. 

```{r moving-average-data}
#| fig-caption: data for moving average example
#| fig-label: data-moving-average

period_data <- learners |>
    mutate(period = row_number() %% 12 + 1) |>
    arrange(period) |>
    mutate(
        month = forcats::fct_inorder(month.abb[period]),
        quarter = forcats::fct_inorder(glue::glue("Q{(period - 1) %/% 3 + 1}"))
    )

period_summary <- period_data |>
    group_by(quarter, month, period) |>
    summarize(
        N = n(),
        n_women_and_beyond = sum(women_and_beyond),
        p_women_and_beyond = mean(women_and_beyond)
    ) |>
    ungroup()


period_summary |>
    ggplot(aes(month, p_women_and_beyond, group = 1)) +
    geom_hline(yintercept = cwss_validation["women_and_beyond"], linetype = "dashed") +
    geom_path() +
    annotate("text", x = "May", y = .51, label = "overall average", hjust = 0) +
    geom_point(aes(fill = quarter), size = 4, shape = 21) +
    scale_y_continuous(limits = c(.25, .75)) +
    bptheme::theme_blueprint()
```

@data-moving-average shows the proportion of `women_and_beyond` learners in each reporting period, along with the overall average for the population. Since the division of the population into reporting periods is arbitrary, the proportion of `women_and_beyond` learners in each period varies somewhat between periods, but cleaves fairly closely to the overall average. 


### calculation

Before we simulate a bunch of rounds of survey responses, I'll calculate the population's moving average, just to demonstrate the process. 

```{r moving-average-example}
#| fig-label: moving-average-example
#| fig-caption: moving average example
library(RcppRoll)
library(ggformula)

moving_average_proportion <- function(N, n, window) {
    roll_sum(n, window, fill = NA, align = "right") / roll_sum(N, window, fill = NA, align = "right")
}

calc_example <- period_summary |>
    mutate(
        three_month_p = moving_average_proportion(N, n_women_and_beyond, 3)
    )


calc_example |>
    ggplot(aes(period, p_women_and_beyond)) +
    geom_point(aes(fill = quarter), size = 4, shape = 21) +
    geom_line(aes(x, y), data = function(d) data.frame(spline(d$period, d$three_month_p, n = 100))) +
    geom_point(aes(y = three_month_p)) +
    scale_y_continuous(limits = c(.25, .75)) +
    scale_x_continuous(breaks = 1:12, labels = function(x) month.abb[x]) +
    bptheme::theme_blueprint()
```

@moving-average-example shows the moving average for the proportion of `women_and_beyond` learners in the population, calculated using a window of three months. 
Using the full set of monthly data, we can see that the three month moving average is smoother than the monthly data, and that it hews yet closer to the overall average across all reporting periods. 
This will remain true for subsequent calculations using simulated survey responses. 

### survey simulation

```{r survey-data}
#| fig-label: survey-data
#| fig-caption: data for survey simulation

sim_monthly_women_and_beyond <- function(
    d, # data frame of learners with period information
    n, # number of survey responses to simulate
    k # number of samples to generate for each reporting period
    ) {
    g <- d |> group_by(month, period)
    seq_len(k) |>
        map(function(i) {
            g |>
                sample_n(n, replace = FALSE) |>
                summarize(N = n, n = sum(women_and_beyond), .groups = "drop") |>
                mutate(iter = i)
        }) |>
        bind_rows() |>
        ungroup()
}

moving_average_samples <- sim_monthly_women_and_beyond(period_data, 15, 200) |>
    group_by(iter) |>
    mutate(
        ma_1 = n / N,
        ma_2 = moving_average_proportion(N, n, 2),
        ma_3 = moving_average_proportion(N, n, 3),
        ma_4 = moving_average_proportion(N, n, 4),
        ma_5 = moving_average_proportion(N, n, 5),
        ma_6 = moving_average_proportion(N, n, 6)
    ) |>
    pivot_longer(
        matches("ma_"),
        names_pattern = "ma_(\\d+)",
        names_to = "window",
        values_to = "p_moving_average"
    ) |>
    filter(!is.na(p_moving_average))

moving_average_samples |>
    ggplot(aes(window, p_moving_average)) +
    stat_slab(aes(fill = window), expand = T, trim = FALSE, height = 3, normalize = "groups", orientation = "vertical") +
    geom_hline(data = calc_example, aes(yintercept = p_women_and_beyond), linetype = "dashed") +
    facet_grid(cols = vars(month), scales = "free_x", space = "free_x") +
    bptheme::theme_blueprint() +
    bpscales::scale_fill_blueprint(discrete = TRUE, type = "multi", option = "fsc") +
    labs(fill = "Number of months of data included in estimate") +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1))
```

@survey-data shows the distributions of estimates of the proportion of `women_and_beyond` learners in the population, calculated for each reporting period using different window sizes. The furthest left distribution in each panel shows the distribution of estimates calculated using only the data from that reporting period, and the distributions to the right show the distributions of estimates calculated using the data from that reporting period and the previous 1, 2, 3, 4, or 5 reporting periods. Because our simulated dataset only includes 12 months of data, the number of months of data available in January is limited to 1, the number in Februrary to 2, and so on. 

The figure clearly shows that, as the window size (and therefore the sample size) increases, the distributions of estimates become more tightly clustered. However, it is also important to recognize that the center of their distribution shifts as well. This is because the population proportion of `women_and_beyond` learners varies somewhat between reporting periods, and so the estimates calculated using data from different reporting periods will vary as well. The trade-off this represents is quite common to statistical analysis: by using a moving average to reduce the error in our estimates, we bias those estimates towards the average of the population over the period of time covered by the moving average.

Using a moving average to borrow sample size from previous reporting periods is suitable for estimating the representation of population characteristics that CLC expects to be relatively stable over time (like gender). However, this approach should be used with caution where representation is expected to change dramatically between periods, especially if / when CLC is attempting to measure the impact of specific interventions.

### more complex moving averages 

The moving average approach outlined above is simple and easy to implement, but it has a few drawbacks, which can be addressed by modifying its implementation. 

- Weights can be applied to the data from each reporting period to place more emphasis on more recent data. This modification mitigates the bias towards the average of the population over the period of time covered by the moving average. 
- The window size can be adjusted to include just enough months of data to achieve an acceptable error. This modification also mitigates the bias toward the window average, but at the cost of a more complex implementation. 

## borrowing information from other sources

As CLC has already recognized, information about the representation of different groups in the local population to which its learners belong is avilable from other sources, including the census and other population surveys. Rather than replacing estimates from small sample sizes entirely, it is possible, as we have seen, to instead _augment_ those estimates with information from other sources. Above, we combined estimates from multiple reporting periods to increase the sample size available for analysis. Here, I'll discuss two approaches to augmenting estimates from small samples with information from public data sources. 

:::{.callout-warning}
## Ground rules for reporting estimates augmented with external data

When reporting estimates like the ones generated by these methods, it is important to be honest and transparent about their construction. Where possible, stick to the following rules: 

- Plainly describe the data and methods used to generate the estimates, preferably with direct links to the data
- Acknowledge the assumptions that underlie the methods
- Justify the assumptions in terms of their relevance to the population of interest
- Address the risk of statistical bias introduced by the external data source alongside your interpretation and analysis

Adherence to these guidelines will help to ensure that the estimates are not misinterpreted or misused, and will safeguard CLC's reputation for rigorous and transparent evaluation.
:::

### a simple weighted average

The simplest way to augment estimates from small samples with information from other sources is to calculate a weighted average of the estimate from the small sample and the estimate from the other source. The basic formula for a weighted average is $\frac{\sum_{i=1}^n w_i x_i}{\sum_{i=1}^n w_i}$, where $x_i$ is the value of the $i$ th observation, and $w_i$ is the weight assigned to that observation. In the context of estimating the representation of a group in the population, the value of $x_i$ is the estimate of the proportion of the group in the sample, and the value of $w_i$ is the weight assigned to that estimate. 

In the simplest example, where CLC is combining a single estimate from a small sample with a single estimate from a public data source, CLC has one choice to make: what weights to use, or how much to 'trust' each estimate. This choice boils down to a judgement call about how much information the public data source provides about the population of learners. In other words, the more that CLC expects the population of learners to resemble the population represented in the public data source, the more weight CLC should assign to the estimate from the public data source.

If we fix the weight for the estimate from the survey data at $n$, where $n$ is the number of responses to the survey, we can understand the weight applied to the public data as the number of learners we want the public data to 'count' for. For example, if we have 5 survey responses, and we want the public data to count for 10 learners, we would assign a weight of 5 to the survey data, and a weight of 10 to the public data. Of course, this would be equivalent to assigning a weight of 1 to the survey data, and a weight of 2 to the public data, but it is conceptually easier to think about the public data as 'counting' for a certain number of learners. 

Imagining the weight as a number of learners, it is intuitive that the weight assigned to the public data should not exceed the number of non-responding learners in the sample. If we have 5 survey responses, and 3 non-responding learners, we can't assign a weight of 10 to the public data, because that would imply that we have 13 learners in the sample, which we don't. The weight must also be greater than or equal to 0, but within this range, the choice of which weight to assign the public data should be based on CLC's conceptual analysis of the relationship between the population of learners and the population represented in the public data.  

:::{.callout-note}
## A straightforward option

The challenge we're attempting to address is having insufficient sample size to produce accurate estimates of the representation of different groups in the learner population. The option that addresses this challenge most directly is to establish a desired margin of error, calculate the required sample size $S$, and then use the public data to 'top up' the sample size to $S$. Plausible values for $S$ range from 100 to 1000, depending on the desired level of precision. 

The approach described above is not -- strictly speaking -- statistically rigorous, because it does not produce a reliable measure of uncertainty. As such, it's not an approach that has received any attention in the statistical literature. It could be readily characterized as within the much underdeveloped field of 'domain knowledge imputation', but it is not a method that has been studied or validated, nor one for which there are established best practices. 

The established best practice is to increase the sample size until the desired margin of error is achieved, and then to report the margin of error along with the estimate. However, this approach is not feasible for CLC, because the sample size is not strictly under CLC's control. 
:::

CLC's current approach replaces unrealiable estimates with either the last available reliable estimate, or with a population estimate. The approach described here is much the same, but it works to _augment_ rather than _replace_ the unreliable estimate. 

### a bayesian regression model

A more statistically rigorous approach to augmenting estimates from small samples with information from other sources is to use a bayesian regression model with 'informative priors' based on the public data. The estimates produced by this approach will be very similar to those produced by the weighted average, but the model will produce measures of uncertainty that can be used to assess and report the reliability of the estimates. 

### relevant public data sources

- [2021 Census Profiles](https://www12.statcan.gc.ca/census-recensement/2021/dp-pd/prof/index.cfm?Lang=E): includes usable population values for low income, rural, newcomer [^immigration-estimates], indigenous, and racialized individuals at the FSA level. Sex representation is also available, but wouldn't be useful for MLEs marketed specifically toward women and beyond (like ladies learning code, girls learning code).
- [2017 Canadian Survey on Disability](https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1310037401): includes usable population values for disability by age group and province. 
- [Monthly Ontario Public School Demographics](https://data.ontario.ca/dataset/school-information-and-student-demographics/resource/e0e90bd5-d662-401a-a6d2-60d69ac89d14): includes newcomer and low income representation among student bodies by school and school board, along with latitude and longitude for each school.


[^immigration-estimates]: CLC currently uses StatCan's [quarterly estimates of the components of population growth](https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1710004001) to estimate the representation of newcomers in the learner population. These estimates don't actually track the representation of newcomers in the population, but rather the number new immigrants to Canada every quarter. Unless I've misunderstood CLC's intentions, these should be replaced with localized estimates of newcomer representation from the 2021 Census.  